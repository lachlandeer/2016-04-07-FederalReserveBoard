{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "user_gender_testset.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN4atdJ/dho/MBBBNBvVn+O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lachlandeer/2016-04-07-FederalReserveBoard/blob/FirstLoad/user_gender_testset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY2rEetrMgk1"
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBZmlIZDNKge",
        "outputId": "adffd558-4f68-41c8-807e-f0409f9cab5e"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Dec 19 22:53:01 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    24W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYDKIu58NM6t"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_vgcoXLNYYC"
      },
      "source": [
        "# Get Data, Model and Tokenizer From Google StorageÂ¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUcrzoemNQfB"
      },
      "source": [
        "project_id = 'egypt-riot-network'\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5RiesP3Ndxi",
        "outputId": "cde5fa29-fcc9-40a1-8917-5203f4022915"
      },
      "source": [
        "!gcloud config set project {project_id}\n",
        "!gsutil ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "\n",
            "\n",
            "To take a quick anonymous survey, run:\n",
            "  $ gcloud survey\n",
            "\n",
            "gs://egypt-riot-network/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk6-7eTKNfUp"
      },
      "source": [
        "!mkdir data model tokenizer\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGUBB8OmNhdS",
        "outputId": "f542373f-2821-469f-b369-015054edef49"
      },
      "source": [
        "!gsutil cp gs://egypt-riot-network/out/data/user_gender/classified_gender_test.csv   data/test_tweets.csv"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://egypt-riot-network/out/data/user_gender/classified_gender_test.csv...\n",
            "- [1 files][255.4 MiB/255.4 MiB]                                                \n",
            "Operation completed over 1 objects/255.4 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dFm6ULwN7pr",
        "outputId": "26b10ae6-1322-4ca1-8ee8-71972fddbeea"
      },
      "source": [
        "# will need to change this - accidently overwrote the pa classifier :/\n",
        "!gsutil cp gs://egypt-riot-network/out/models/user_pa_classifier/model/model/* model/.\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://egypt-riot-network/out/models/user_pa_classifier/model/model/config.json...\n",
            "Copying gs://egypt-riot-network/out/models/user_pa_classifier/model/model/pytorch_model.bin...\n",
            "- [2 files][515.8 MiB/515.8 MiB]                                                \n",
            "Operation completed over 2 objects/515.8 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRAn48hVOBNS",
        "outputId": "438bfb09-3859-4557-f549-9ffa253d9623"
      },
      "source": [
        "!gsutil cp gs://egypt-riot-network/out/models/user_pa_classifier/tokenizer/tokenizer/* tokenizer/.\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://egypt-riot-network/out/models/user_pa_classifier/tokenizer/tokenizer/special_tokens_map.json...\n",
            "Copying gs://egypt-riot-network/out/models/user_pa_classifier/tokenizer/tokenizer/tokenizer_config.json...\n",
            "Copying gs://egypt-riot-network/out/models/user_pa_classifier/tokenizer/tokenizer/vocab.txt...\n",
            "- [3 files][638.8 KiB/638.8 KiB]                                                \n",
            "Operation completed over 3 objects/638.8 KiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NMq1CsDPL27"
      },
      "source": [
        "# Load data to attach predictions to\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCxoQuA1PIAN"
      },
      "source": [
        "df = pd.read_csv('data/test_tweets.csv')\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "qACH_ls4PU_D",
        "outputId": "9de407ab-850c-4a82-e1ba-f2a49e731d7d"
      },
      "source": [
        "df.head()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>id</th>\n",
              "      <th>text_seg</th>\n",
              "      <th>gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000437560</td>\n",
              "      <td>306195358747328512</td>\n",
              "      <td>[ÙØ³ØªØ®Ø¯Ù] ÙÙÙØ§ +Øª Ù+ ÙÙØ³ +Øª Ù+ Ø§Ù+ ÙÙÙ +Ø§Øª ÙÙ Ù...</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000437560</td>\n",
              "      <td>295864428795138050</td>\n",
              "      <td>Ø§Ø­ +ÙØ§ +ÙÙ +ÙØ§ Ø¨+ ÙØ·Ø§ÙØ¨ Ø¨+ ÙØµØ§ÙØ­ +Ø§Øª Ø¬ÙÙØ¨ Ø§ÙØ±Ù...</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000437560</td>\n",
              "      <td>295864097239597058</td>\n",
              "      <td>ÙØµØ± Ø§Ù+ ÙÙÙ ØªØ­ØªØ±Ù</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000437560</td>\n",
              "      <td>283615531507019777</td>\n",
              "      <td>Ù+ Ø³ÙÙ ÙØ³ØªÙØªÙ Ø¨Ø§ÙÙ Ø§Ù+ Ø´Ø¹Ø¨ Ø¹ÙÙ ØªØ´ÙÙÙ ÙØ®Ø§ÙÙ Ø§Ù+...</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000437560</td>\n",
              "      <td>283615278863101952</td>\n",
              "      <td>Ø­Ø§Ù ÙÙÙ ÙØ¬ÙØ³ Ø§Ù+ Ø´ÙØ±Ù Ø¨+ ØªØ´ÙÙÙ +Ù Ø§Ù+ Ø­Ø§ÙÙ ÙØ¨Ù...</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      user_id  ...  gender\n",
              "0  1000437560  ...    male\n",
              "1  1000437560  ...    male\n",
              "2  1000437560  ...    male\n",
              "3  1000437560  ...    male\n",
              "4  1000437560  ...    male\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF21Ds5SPXOi"
      },
      "source": [
        "DATA_COLUMN = 'text'\n",
        "ID_COLUMN   = 'id'\n",
        "\n",
        "df_test = df[['id', 'text_seg']]\n",
        "df_test.columns = [ID_COLUMN, DATA_COLUMN]\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "1I0L7nT2QCx1",
        "outputId": "6d3b303a-3933-4abb-d1fe-22834a959141"
      },
      "source": [
        "df_test.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>306195358747328512</td>\n",
              "      <td>[ÙØ³ØªØ®Ø¯Ù] ÙÙÙØ§ +Øª Ù+ ÙÙØ³ +Øª Ù+ Ø§Ù+ ÙÙÙ +Ø§Øª ÙÙ Ù...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>295864428795138050</td>\n",
              "      <td>Ø§Ø­ +ÙØ§ +ÙÙ +ÙØ§ Ø¨+ ÙØ·Ø§ÙØ¨ Ø¨+ ÙØµØ§ÙØ­ +Ø§Øª Ø¬ÙÙØ¨ Ø§ÙØ±Ù...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>295864097239597058</td>\n",
              "      <td>ÙØµØ± Ø§Ù+ ÙÙÙ ØªØ­ØªØ±Ù</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>283615531507019777</td>\n",
              "      <td>Ù+ Ø³ÙÙ ÙØ³ØªÙØªÙ Ø¨Ø§ÙÙ Ø§Ù+ Ø´Ø¹Ø¨ Ø¹ÙÙ ØªØ´ÙÙÙ ÙØ®Ø§ÙÙ Ø§Ù+...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>283615278863101952</td>\n",
              "      <td>Ø­Ø§Ù ÙÙÙ ÙØ¬ÙØ³ Ø§Ù+ Ø´ÙØ±Ù Ø¨+ ØªØ´ÙÙÙ +Ù Ø§Ù+ Ø­Ø§ÙÙ ÙØ¨Ù...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   id                                               text\n",
              "0  306195358747328512  [ÙØ³ØªØ®Ø¯Ù] ÙÙÙØ§ +Øª Ù+ ÙÙØ³ +Øª Ù+ Ø§Ù+ ÙÙÙ +Ø§Øª ÙÙ Ù...\n",
              "1  295864428795138050  Ø§Ø­ +ÙØ§ +ÙÙ +ÙØ§ Ø¨+ ÙØ·Ø§ÙØ¨ Ø¨+ ÙØµØ§ÙØ­ +Ø§Øª Ø¬ÙÙØ¨ Ø§ÙØ±Ù...\n",
              "2  295864097239597058                                  ÙØµØ± Ø§Ù+ ÙÙÙ ØªØ­ØªØ±Ù\n",
              "3  283615531507019777  Ù+ Ø³ÙÙ ÙØ³ØªÙØªÙ Ø¨Ø§ÙÙ Ø§Ù+ Ø´Ø¹Ø¨ Ø¹ÙÙ ØªØ´ÙÙÙ ÙØ®Ø§ÙÙ Ø§Ù+...\n",
              "4  283615278863101952  Ø­Ø§Ù ÙÙÙ ÙØ¬ÙØ³ Ø§Ù+ Ø´ÙØ±Ù Ø¨+ ØªØ´ÙÙÙ +Ù Ø§Ù+ Ø­Ø§ÙÙ ÙØ¨Ù..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evMj7B8DQEZH"
      },
      "source": [
        "!mkdir out_data\n",
        "df_test.to_csv(\"out_data/test.tsv\",index=False,columns=df_test.columns,sep='\\t',header=False)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWwyBMG-QOts"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yZBAE5HQLud",
        "outputId": "a38020ef-bad9-428a-d602-eb009a878960"
      },
      "source": [
        "!pip install farasapy\n",
        "!pip install pyarabic\n",
        "!git clone https://github.com/aub-mind/arabert\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting farasapy\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/49/a1cea02059325e99bbd1114704140101893c629e67b15eaacb93711dc91e/farasapy-0.0.11-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from farasapy) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from farasapy) (4.41.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (1.24.3)\n",
            "Installing collected packages: farasapy\n",
            "Successfully installed farasapy-0.0.11\n",
            "Collecting pyarabic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/e2/46728ec2f6fe14970de5c782346609f0636262c0941228f363710903aaa1/PyArabic-0.6.10.tar.gz (108kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 112kB 13.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyarabic\n",
            "  Building wheel for pyarabic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyarabic: filename=PyArabic-0.6.10-cp36-none-any.whl size=113324 sha256=26d05ea45fc846f8e1f947191f0ffc9c8d8776c17e1a607f706388f622b3958e\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/b8/f5/b7c1a50e6efb83544844f165a9b134afe7292585465e29b61d\n",
            "Successfully built pyarabic\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.10\n",
            "Cloning into 'arabert'...\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 279 (delta 34), reused 38 (delta 15), pack-reused 214\u001b[K\n",
            "Receiving objects: 100% (279/279), 3.68 MiB | 965.00 KiB/s, done.\n",
            "Resolving deltas: 100% (157/157), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8ABh4o2QYA_",
        "outputId": "5462f7a1-27c1-4745-aff8-c9c39379bcb0"
      },
      "source": [
        "!pip install torch\n",
        "!pip install transformers"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 1.5MB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 890kB 62.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 2.9MB 49.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=700df6bb6430407b8a262749cb4e5811584d57320fd26bb1d460f590769303db\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuZAAxpuQZxJ"
      },
      "source": [
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import csv\n",
        "import sys\n",
        "from io import open\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm_notebook, trange\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "from transformers import BertPreTrainedModel\n",
        "from transformers import BertModel\n",
        "from transformers import WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer                                  \n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from arabert.preprocess_arabert import never_split_tokens\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmoELfthQdRK"
      },
      "source": [
        "from transformers import BertForSequenceClassification\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Uj65DYmQg-f"
      },
      "source": [
        "# Data Loading Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye7_myHhRJ-y"
      },
      "source": [
        "\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" BERT classification fine-tuning: utilities to work with GLUE tasks \"\"\"\n",
        "csv.field_size_limit(2147483647)\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_predict_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                if sys.version_info[0] == 2:\n",
        "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "\n",
        "class BinaryProcessor(DataProcessor):\n",
        "    \"\"\"Processor for the binary data sets\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir , train_file_name):\n",
        "        \"\"\"See base class. file should be a tsv\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, train_file_name)), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir, dev_file_name):\n",
        "        \"\"\"See base class. file should be a tsv\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, dev_file_name)), \"dev\")\n",
        "\n",
        "    def get_predict_examples(self, data_dir, train_file_name):\n",
        "        \"\"\"See base class. file should be a tsv\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, train_file_name)), \"predict\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return [\"0\", \"1\"]\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "\n",
        "        if(set_type!=\"predict\"):\n",
        "            for (i, line) in enumerate(lines):\n",
        "                guid = \"%s-%s\" % (set_type, i)\n",
        "                text_a = line[3]\n",
        "                label = line[1]\n",
        "                examples.append(\n",
        "                    InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        else:\n",
        "            for (i, line) in enumerate(lines):\n",
        "                guid = \"%s-%s\" % (set_type, i)\n",
        "                text_a = line[1]\n",
        "                label = '0'\n",
        "                examples.append(\n",
        "                    InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "def convert_example_to_feature(example_row, pad_token=0,\n",
        "sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
        "cls_token_segment_id=1, pad_token_segment_id=0,\n",
        "mask_padding_with_zero=True, sep_token_extra=False):\n",
        "    example, label_map, max_seq_length, tokenizer, output_mode, cls_token_at_end, cls_token, sep_token, cls_token_segment_id, pad_on_left, pad_token_segment_id, sep_token_extra = example_row\n",
        "\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "    tokens_b = None\n",
        "    if example.text_b:\n",
        "        tokens_b = tokenizer.tokenize(example.text_b)\n",
        "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "        # length is less than the specified length.\n",
        "        # Account for [CLS], [SEP], [SEP] with \"- 3\". \" -4\" for RoBERTa.\n",
        "        special_tokens_count = 4 if sep_token_extra else 3\n",
        "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - special_tokens_count)\n",
        "    else:\n",
        "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
        "        special_tokens_count = 3 if sep_token_extra else 2\n",
        "        if len(tokens_a) > max_seq_length - special_tokens_count:\n",
        "            tokens_a = tokens_a[:(max_seq_length - special_tokens_count)]\n",
        "\n",
        "    # The convention in BERT is:\n",
        "    # (a) For sequence pairs:\n",
        "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "    #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
        "    # (b) For single sequences:\n",
        "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "    #  type_ids:   0   0   0   0  0     0   0\n",
        "    #\n",
        "    # Where \"type_ids\" are used to indicate whether this is the first\n",
        "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "    # embedding vector (and position vector). This is not *strictly* necessary\n",
        "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "    # it easier for the model to learn the concept of sequences.\n",
        "    #\n",
        "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "    # the entire model is fine-tuned.\n",
        "    tokens = tokens_a + [sep_token]\n",
        "    segment_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "    if tokens_b:\n",
        "        tokens += tokens_b + [sep_token]\n",
        "        segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
        "\n",
        "    if cls_token_at_end:\n",
        "        tokens = tokens + [cls_token]\n",
        "        segment_ids = segment_ids + [cls_token_segment_id]\n",
        "    else:\n",
        "        tokens = [cls_token] + tokens\n",
        "        segment_ids = [cls_token_segment_id] + segment_ids\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    padding_length = max_seq_length - len(input_ids)\n",
        "    if pad_on_left:\n",
        "        input_ids = ([pad_token] * padding_length) + input_ids\n",
        "        input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
        "        segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
        "    else:\n",
        "        input_ids = input_ids + ([pad_token] * padding_length)\n",
        "        input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "        segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "\n",
        "    if output_mode == \"classification\":\n",
        "        label_id = label_map[example.label]\n",
        "    elif output_mode == \"regression\":\n",
        "        label_id = float(example.label)\n",
        "    else:\n",
        "        raise KeyError(output_mode)\n",
        "\n",
        "    return InputFeatures(input_ids=input_ids,\n",
        "                        input_mask=input_mask,\n",
        "                        segment_ids=segment_ids,\n",
        "                        label_id=label_id)\n",
        "  \n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length,\n",
        "                                 tokenizer, output_mode,\n",
        "                                 cls_token_at_end=False, sep_token_extra=False, pad_on_left=False,\n",
        "                                 cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
        "                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
        "                                 cls_token_segment_id=1, pad_token_segment_id=0,\n",
        "                                 mask_padding_with_zero=True):\n",
        "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
        "        `cls_token_at_end` define the location of the CLS token:\n",
        "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
        "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
        "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
        "    \"\"\"\n",
        "\n",
        "    label_map = {label : i for i, label in enumerate(label_list)}\n",
        "\n",
        "    examples = [(example, label_map, max_seq_length, tokenizer, output_mode, cls_token_at_end, cls_token, sep_token, cls_token_segment_id, pad_on_left, pad_token_segment_id, sep_token_extra) for example in examples]\n",
        "\n",
        "    process_count = cpu_count()\n",
        "\n",
        "    with Pool(process_count) as p:\n",
        "        features = list(tqdm(p.imap(convert_example_to_feature, examples, chunksize=500), total=len(examples)))\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "\n",
        "processors = {\n",
        "    \"binary\": BinaryProcessor\n",
        "}\n",
        "\n",
        "output_modes = {\n",
        "    \"binary\": \"classification\"\n",
        "}"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3gqDnNqRXYd"
      },
      "source": [
        "def load_and_cache_examples(task, tokenizer, mode=\"train\"):\n",
        "    processor = processors[task]()\n",
        "    output_mode = args['output_mode']\n",
        "    \n",
        "    cached_features_file = os.path.join(args['data_dir'], f\"cached_{mode}_{args['cache_dir']}_{args['max_seq_length']}_{task}\")\n",
        "    \n",
        "    if os.path.exists(cached_features_file) and not args['reprocess_input_data']:\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "               \n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", args['data_dir'])\n",
        "        label_list = processor.get_labels()\n",
        "        if mode==\"train\":\n",
        "          examples=processor.get_train_examples(args['data_dir'],args['train_file_name']);\n",
        "        if mode==\"dev\":\n",
        "          examples=processor.get_dev_examples(args['data_dir'],args['dev_file_name']);\n",
        "        if mode==\"predict\":\n",
        "          examples=processor.get_predict_examples(args['data_dir'],args['pred_file_name'])\n",
        "        \n",
        "        if __name__ == \"__main__\":\n",
        "            features = convert_examples_to_features(examples, label_list, args['max_seq_length'], tokenizer, output_mode,\n",
        "                cls_token_at_end=bool(args['model_type'] in ['xlnet']),            # xlnet has a cls token at the end\n",
        "                cls_token=tokenizer.cls_token,\n",
        "                cls_token_segment_id=2 if args['model_type'] in ['xlnet'] else 0,\n",
        "                sep_token=tokenizer.sep_token,\n",
        "                sep_token_extra=bool(args['model_type'] in ['roberta']),           # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n",
        "                pad_on_left=bool(args['model_type'] in ['xlnet']),                 # pad on the left for xlnet\n",
        "                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
        "                pad_token_segment_id=4 if args['model_type'] in ['xlnet'] else 0)\n",
        "        \n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save(features, cached_features_file)\n",
        "        \n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "    if output_mode == \"classification\":\n",
        "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
        "    elif output_mode == \"regression\":\n",
        "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "    return dataset"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozqrK8gwRdUx"
      },
      "source": [
        "# Predict Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmfWJweyRcZz"
      },
      "source": [
        "def predict(model, tokenizer, prefix=\"\"):\n",
        "  pred_output_dir = args['output_dir']\n",
        "  \n",
        "  results={}\n",
        "  PRED_TASK = args['task_name']\n",
        "  model.eval()\n",
        "  \n",
        "  pred_dataset = load_and_cache_examples(PRED_TASK, tokenizer, mode='predict')\n",
        "  if not os.path.exists(pred_output_dir):\n",
        "    os.makedirs(pred_output_dir)\n",
        "    \n",
        "  \n",
        "  pred_sampler = SequentialSampler(pred_dataset)\n",
        "  pred_dataloader = DataLoader(pred_dataset, sampler=pred_sampler, batch_size=args['eval_batch_size'])\n",
        "  \n",
        "  logger.info(\"***** Running prediction {} *****\".format(prefix))\n",
        "  logger.info(\"  Num examples = %d\", len(pred_dataset))\n",
        "  logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
        "  \n",
        "  preds = None\n",
        "  for batch in pred_dataloader:\n",
        "    with torch.no_grad():\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      inputs = {'input_ids': batch[0],'attention_mask': batch[1],'token_type_ids': batch[2],'labels': batch[3]}\n",
        "      \n",
        "      outputs = model(**inputs)\n",
        "      _, logits = outputs[:2]\n",
        "    if preds is None:\n",
        "        preds = logits.detach().cpu().numpy()\n",
        "    else:\n",
        "        preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "\n",
        "    #torch.cuda.empty_cache()        \n",
        "\n",
        "  preds = np.argmax(preds, axis=1)\n",
        "      \n",
        "  return preds"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MdyAL3XRiNC"
      },
      "source": [
        "# Define Model Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l9xMHcQRg_J"
      },
      "source": [
        "args = {\n",
        "    'data_dir': 'out_data/',\n",
        "    'train_file_name': 'train.tsv',\n",
        "    'dev_file_name': 'dev.tsv',\n",
        "    'pred_file_name': 'test.tsv',\n",
        "    'model_type':  'bert',\n",
        "    'model_name': 'aubmindlab/bert-base-arabert',\n",
        "    'task_name': 'binary',\n",
        "    'output_dir': 'outputs_bert/',\n",
        "    'cache_dir': 'cache',\n",
        "    'do_train': False,\n",
        "    'do_eval': True,\n",
        "    'fp16': False,\n",
        "    'fp16_opt_level': 'O1',\n",
        "    'max_seq_length': 128,\n",
        "    'output_mode': 'classification',\n",
        "    'train_batch_size': 16,\n",
        "    'eval_batch_size': 32,\n",
        "    'num_train_epochs': 3,\n",
        "    'weight_decay': 0,\n",
        "    'learning_rate': 2e-5,\n",
        "    'adam_epsilon': 1e-8,\n",
        "    'warmup_steps': 0,\n",
        "    'max_grad_norm': 1.0,\n",
        "    'log_dir':'./logs',\n",
        "    'logging_steps': 0,\n",
        "    'evaluate_during_training': True,\n",
        "    'save_steps': 90,\n",
        "    'eval_all_checkpoints': True,\n",
        "    'overwrite_output_dir': True,\n",
        "    'reprocess_input_data': False,\n",
        "    'notes': 'action_classification_arabert'\n",
        "}\n",
        "with open('args.json', 'w') as f:\n",
        "    json.dump(args, f)\n",
        "\n",
        "!mkdir ./{args['log_dir']}"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYHOcYwNRnmX"
      },
      "source": [
        "# Create & Configure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue4O29vWRmuk"
      },
      "source": [
        "MODEL_CLASSES = {\n",
        "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
        "    #'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
        "    #'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
        "    #'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "}\n",
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[args['model_type']]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NzEhPBlRseG",
        "outputId": "460aa24d-b874-483a-8087-31a14bb2ab0c"
      },
      "source": [
        "#Checking for GPU\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "    !nvidia-smi\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla V100-SXM2-16GB\n",
            "Sat Dec 19 23:12:50 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    24W / 300W |     10MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kEtKxCER-j6"
      },
      "source": [
        "# Load Tokenizer and Model From File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qluqYZ4RvCH"
      },
      "source": [
        "#https://github.com/huggingface/transformers/blob/master/src/transformers/configuration_bert.py#L52\n",
        "#config = config_class.from_pretrained(args['model_name'], num_labels=2, finetuning_task=args['task_name'])\n",
        "\n",
        "model = model_class.from_pretrained('model/', output_attentions=True)\n",
        "model.to(device)\n",
        "\n",
        "#https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_bert.py#L119\n",
        "tokenizer = tokenizer_class.from_pretrained('tokenizer/',\n",
        "    do_lower_case=False,\n",
        "    do_basic_tokenize=True,\n",
        "    never_split=never_split_tokens)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCAcV-q5SEsc"
      },
      "source": [
        "task = args['task_name']\n",
        "\n",
        "if task in processors.keys() and task in output_modes.keys():\n",
        "    processor = processors[task]()\n",
        "    label_list = processor.get_labels()\n",
        "    num_labels = len(label_list)\n",
        "else:\n",
        "    raise KeyError(f'{task} not found in processors or in output_modes. Please check utils.py.')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2860ZdTSNTz"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1MGubFkSMAs",
        "outputId": "f94d2f9d-45ee-4239-c87d-a9b0396c2848"
      },
      "source": [
        "preds = predict(model,tokenizer)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Creating features from dataset file at out_data/\n",
            "100%|ââââââââââ| 1404013/1404013 [05:22<00:00, 4351.73it/s]\n",
            "INFO:__main__:Saving features into cached file out_data/cached_predict_cache_128_binary\n",
            "INFO:__main__:***** Running prediction  *****\n",
            "INFO:__main__:  Num examples = 1404013\n",
            "INFO:__main__:  Batch size = 32\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MceXrsYSXrE"
      },
      "source": [
        "Join Predictions to Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5pVOM78SJb-"
      },
      "source": [
        "df_predictions = pd.concat((df.reset_index(),\n",
        "                            pd.DataFrame(preds,columns=['predicted'])\n",
        "                            ),\n",
        "                           axis=1,\n",
        "                           ignore_index=False\n",
        "                           )"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "Zf9uI65bZ38k",
        "outputId": "3ca63d02-131c-4d44-b852-a1211ce5a450"
      },
      "source": [
        "df_predictions.head(n = 20)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>user_id</th>\n",
              "      <th>id</th>\n",
              "      <th>text_seg</th>\n",
              "      <th>gender</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>306195358747328512</td>\n",
              "      <td>[ÙØ³ØªØ®Ø¯Ù] ÙÙÙØ§ +Øª Ù+ ÙÙØ³ +Øª Ù+ Ø§Ù+ ÙÙÙ +Ø§Øª ÙÙ Ù...</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>295864428795138050</td>\n",
              "      <td>Ø§Ø­ +ÙØ§ +ÙÙ +ÙØ§ Ø¨+ ÙØ·Ø§ÙØ¨ Ø¨+ ÙØµØ§ÙØ­ +Ø§Øª Ø¬ÙÙØ¨ Ø§ÙØ±Ù...</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>295864097239597058</td>\n",
              "      <td>ÙØµØ± Ø§Ù+ ÙÙÙ ØªØ­ØªØ±Ù</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>283615531507019777</td>\n",
              "      <td>Ù+ Ø³ÙÙ ÙØ³ØªÙØªÙ Ø¨Ø§ÙÙ Ø§Ù+ Ø´Ø¹Ø¨ Ø¹ÙÙ ØªØ´ÙÙÙ ÙØ®Ø§ÙÙ Ø§Ù+...</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>283615278863101952</td>\n",
              "      <td>Ø­Ø§Ù ÙÙÙ ÙØ¬ÙØ³ Ø§Ù+ Ø´ÙØ±Ù Ø¨+ ØªØ´ÙÙÙ +Ù Ø§Ù+ Ø­Ø§ÙÙ ÙØ¨Ù...</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>283614855443935232</td>\n",
              "      <td>Ù+ ÙØ¹ Ø§Ù+ ØªØ³ÙÙÙ Ø¬Ø¯Ù +Ø§ Ø¨Ø§Ù Ø§Ù+ Ø¹Ø¨Ø± +Ø© Ø¨+ Ø¹Ø±Ø¶ +...</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>283614603861184512</td>\n",
              "      <td>Ø§Ù+ Ø­Ø§ÙÙ Ù+ ÙØ§ ÙØ¬ÙØ² ØªØ¹Ø¯ÙÙ ØªØ´ÙÙÙ +Ù Ø¨Ø¹Ø¯ Ø§Ø¹Ø¯Ø§Ø¯ Ù...</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>283614455911305216</td>\n",
              "      <td>Ø¹ÙÙ Ø£Ù ÙØªÙ Ø°ÙÙ Ø®ÙØ§Ù Ø³Ù +Ø© ÙÙ ØªØ§Ø±ÙØ® Ø§ÙØ¹ÙØ§Ø¯ ÙØ¬ÙØ³...</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>283613874538815490</td>\n",
              "      <td>Ù+ ØªÙØªÙÙ Ø¥ÙÙ ÙØ¬ÙØ³ Ø§Ù+ ÙÙØ§Ø¨ Ø ÙÙØ± Ø§ÙØªØ®Ø§Ø¨ +Ù Ø Ø§...</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>283613463782252545</td>\n",
              "      <td>ÙØªÙÙÙ ÙØ¬ÙØ³ Ø§Ù+ Ø´ÙØ±Ù Ø§Ù+ ÙØ§Ø¦Ù Ø¨+ ØªØ´ÙÙÙ +Ù Ø§Ù+ Ø­...</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>283613182373789697</td>\n",
              "      <td>ØªØ¹ÙÙÙ Ø§Ù+ Ø±Ø¦ÙØ³ Ù+ Ø£Ø¹Ø¶Ø§Ø¡ ÙØ¬ÙØ³ Ø§Ù+ Ø´ÙØ±Ù Ø§Ù+ ØªØ³Ø¹ ...</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>282274336063229952</td>\n",
              "      <td>Ø§Ù+ ÙÙÙ Ø§Ù+ Ø³Ø§Ø¨Ø¹ | Ø±Ø¦ÙØ³ ÙØ§Ø¯Ù ÙØ¶Ø§ +Ø© Ø§Ù+ ÙÙÙÙÙ ...</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>281917852271919105</td>\n",
              "      <td>ÙÙØ¹ÙØ¯ Ø§Ù+ ÙØ¶Ø§Ø¡ ÙÙ ÙÙÙ Ø®ÙÙØ³ Ø¨+ ÙØµÙØ¨ +Ù</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>281917376193253376</td>\n",
              "      <td>Ù+ ÙÙ ÙØ®Ø¶Ø¹ Ù+ Ø§Ù+ Ø§ÙØ±Ø§Ù ÙÙØ®Ø´Ø§Ù Ø³+ ÙØ®Ø¶Ø¹ ÙÙÙØ§ ÙØ¬...</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>281917161067393024</td>\n",
              "      <td>ÙÙ ÙØ®Ø¶Ø¹ Ù+ Ø§Ù+ Ø§ÙØ±Ø§Ù ÙØ§ ÙØµÙØ­ Ù+ Ø§Ù+ ÙØ¶Ø§Ø¡</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>281917051088560128</td>\n",
              "      <td>ÙØ§ Ø³Ø¨Ø¨ ØªÙØ³Ù Ø§Ù+ Ø§Ø®ÙØ§Ù Ø¨+ Ø§Ù+ ÙØ§Ø¦Ø¨ Ø§Ù+ Ø¹Ø§Ù Ø¨+ Ù...</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>281754331533963265</td>\n",
              "      <td>Ø§Ù+ Ø­ÙØ¯ Ù+ Ø§ÙÙÙ +Ø§Ù ÙØ¬ÙØ³ Ø§Ù+ ÙØ¶Ø§Ø¡ ÙÙ ÙØ¨Øª ÙÙ ÙÙ...</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>280833768858726400</td>\n",
              "      <td>Ø§Ù+ ÙÙÙ Ø£Ø³ÙØ· Ø±Ø¬Ø§Ù Ø§Ù+ ÙØ¶Ø§Ø¡ Ø§Ù+ ÙØµØ±Ù Ø§Ù+ ÙØ§Ø¦Ø¨ Ø§...</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>280656971387002881</td>\n",
              "      <td>Ø¨Ø¯Ø¡ Ø§Ø¬ØªÙØ§Ø¹ Ø£ÙØ«Ø± ÙÙ Ø£ÙÙ Ø¹Ø¶Ù ÙÙØ§Ø¨ +Ù ÙØ¹ Ø±Ø¦ÙØ³ ÙØ¬Ù...</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>1000437560</td>\n",
              "      <td>280656575318851584</td>\n",
              "      <td>+Ø§Ù Ø§Ù+ ÙØ±Ø§Ø³Ù ÙØ§ ØªØºÙØ± Ø§Ù+ Ø£Ø´Ø®Ø§Øµ Ù+ ÙÙÙ ØªØ­Ø·Ù ØªÙ...</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index     user_id  ...  gender predicted\n",
              "0       0  1000437560  ...    male         0\n",
              "1       1  1000437560  ...    male         1\n",
              "2       2  1000437560  ...    male         0\n",
              "3       3  1000437560  ...    male         0\n",
              "4       4  1000437560  ...    male         0\n",
              "5       5  1000437560  ...    male         0\n",
              "6       6  1000437560  ...    male         1\n",
              "7       7  1000437560  ...    male         0\n",
              "8       8  1000437560  ...    male         0\n",
              "9       9  1000437560  ...    male         1\n",
              "10     10  1000437560  ...    male         0\n",
              "11     11  1000437560  ...    male         0\n",
              "12     12  1000437560  ...    male         0\n",
              "13     13  1000437560  ...    male         1\n",
              "14     14  1000437560  ...    male         1\n",
              "15     15  1000437560  ...    male         0\n",
              "16     16  1000437560  ...    male         0\n",
              "17     17  1000437560  ...    male         1\n",
              "18     18  1000437560  ...    male         1\n",
              "19     19  1000437560  ...    male         1\n",
              "\n",
              "[20 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "qr5x8hm8Z6DR",
        "outputId": "726dcf4c-d85b-4ac0-a958-82eaa0e90914"
      },
      "source": [
        "cols_keep = ['user_id', 'gender', 'predicted']\n",
        "df_predictions = df_predictions[cols_keep]\n",
        "\n",
        "df_predictions.head()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>gender</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000437560</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000437560</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000437560</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000437560</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000437560</td>\n",
              "      <td>male</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      user_id gender  predicted\n",
              "0  1000437560   male          0\n",
              "1  1000437560   male          1\n",
              "2  1000437560   male          0\n",
              "3  1000437560   male          0\n",
              "4  1000437560   male          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SchQ-9SfaLch"
      },
      "source": [
        "# Recode pol affil to binary to compare to predictionsÂ¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-UXM_q4aIS_"
      },
      "source": [
        "label_map = {\n",
        "    'male' : 0,\n",
        "    'female' : 1\n",
        "}\n",
        "\n",
        "df_predictions['gender'] = df_predictions['gender'].apply(lambda x: label_map[x])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjyplO5oaUDM"
      },
      "source": [
        "#https://stackoverflow.com/questions/32459325/python-pandas-dataframe-select-row-by-max-value-in-group\n",
        "user_tweet_type_count = df_predictions.groupby(['user_id', 'predicted']).agg(lambda x:x.value_counts()).reset_index()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8nFVF4eabOW"
      },
      "source": [
        "user_preds = user_tweet_type_count.loc[user_tweet_type_count.reset_index().groupby(['user_id'])['gender'].idxmax()]\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUuY2y9Oak4z",
        "outputId": "c33027a8-cbe7-4ec0-8d14-a15cc33e71e0"
      },
      "source": [
        "user_preds.groupby(['predicted']).size()\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "predicted\n",
              "0    1069\n",
              "1     493\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwelDAWYap2N"
      },
      "source": [
        "user_preds.drop(['gender'], inplace = True, axis = 1)\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dsft0vopa3Rw"
      },
      "source": [
        "# Load Ground Truth Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDKcevZAazMJ",
        "outputId": "8bd0786d-ed60-4d42-aac4-48f5675eac43"
      },
      "source": [
        "!gsutil cp gs://egypt-riot-network/out/data/user_gender/classified_gender_test_users.csv data/users_test.csv\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://egypt-riot-network/out/data/user_gender/classified_gender_test_users.csv...\n",
            "/ [1 files][ 23.3 KiB/ 23.3 KiB]                                                \n",
            "Operation completed over 1 objects/23.3 KiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cQg6xNcbRkX"
      },
      "source": [
        "user_types = pd.read_csv('data/users_test.csv')\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3Nd49wIbYF2"
      },
      "source": [
        "user_types['gender'] = user_types['gender'].apply(lambda x: label_map[x])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpE7kMq8bfBD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPthw2eNbjQU"
      },
      "source": [
        "# Merge Predictions & Truth Together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5BWbUJdblaO"
      },
      "source": [
        "user_comparison = user_types.set_index('user_id').join(user_preds.set_index('user_id'), how = 'inner')\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-KCj6IIbonR"
      },
      "source": [
        ""
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRHylmdrbr-u"
      },
      "source": [
        "# Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMoPlyfRbtmv"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNoqzxvsbxiS",
        "outputId": "7d388278-6ac7-44d3-fcdf-ec26be3e7263"
      },
      "source": [
        "confusion_matrix(user_comparison['gender'], user_comparison['predicted'])\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1008,  285],\n",
              "       [  61,  208]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFV8bGyib1tc",
        "outputId": "2a1b4cff-e25f-4582-9eed-f2b81dfc5fcd"
      },
      "source": [
        "print(classification_report(user_comparison['gender'], user_comparison['predicted']))\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.78      0.85      1293\n",
            "           1       0.42      0.77      0.55       269\n",
            "\n",
            "    accuracy                           0.78      1562\n",
            "   macro avg       0.68      0.78      0.70      1562\n",
            "weighted avg       0.85      0.78      0.80      1562\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_YLaL7UdPXB"
      },
      "source": [
        "# Save Predictions Next to Truths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7WcFx2SdSuD"
      },
      "source": [
        "user_comparison.to_csv('out_data/predictions_arabert_testset.csv',\n",
        "                                index = False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5Byl9a5dnTM"
      },
      "source": [
        "!gsutil cp -r out_data/predictions_arabert_testset.csv gs://egypt-riot-network/out/data/user_gender\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}